# -*- coding: utf-8 -*-
"""
ê´‘ê³ ì—…ê³„ íŠ¸ë Œë“œ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (v6.1 - ì •ì œ ê°•í™”)

í•µì‹¬ ë³€ê²½:
- AdAge/Digiday ì‚¬ì´íŠ¸ ê³µí†µ ë¬¸êµ¬(ì¡ìŒ) ê°•ë ¥ ì œê±°
- í•™ìˆ  ì—°êµ¬/ë…¼ë¬¸ ì„¹ì…˜ ë³„ë„ ìœ ì§€
- ìš”ì•½ ê¸¸ì´ ë° ë¬¸ì¥ ì™„ê²°ì„± ë¡œì§ ìœ ì§€
"""

import json
import logging
import re
import time
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from config import (
    TREND_CATEGORIES,
    get_today_data_dir,
    get_today_str,
    get_today_report_path,
)

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë²ˆì—­ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from deep_translator import GoogleTranslator
    translator = GoogleTranslator(source='auto', target='ko')
    HAS_TRANSLATOR = True
except ImportError:
    HAS_TRANSLATOR = False
    logger.warning("deep-translator ë¯¸ì„¤ì¹˜")


def translate(text):
    if not text or not HAS_TRANSLATOR:
        return text
    try:
        if len(text) > 4500:
            text = text[:4500]
        result = translator.translate(text)
        time.sleep(0.5)
        return result if result else text
    except Exception as e:
        logger.debug(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì›ë¬¸ ê¸°ì‚¬ ì „ë¬¸ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


def fetch_article_fulltext(url):
    if not url: return ""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "html.parser")

        for tag in soup.find_all(["script", "style", "nav", "footer", "header",
                                   "aside", "iframe", "noscript", "form", "svg", "button"]):
            tag.decompose()

        body_text = ""
        article = soup.find("article")
        if article:
            body_text = article.get_text(separator="\n", strip=True)
        
        if len(body_text) < 500:
            for selector in [
                ".article-content", ".post-content", ".entry-content",
                ".story-body", ".article-body", ".content-body",
                '[itemprop="articleBody"]', ".article__body",
                "#article-body", ".node-body", ".wysiwyg"
            ]:
                el = soup.select_one(selector)
                if el:
                    body_text = el.get_text(separator="\n", strip=True)
                    if len(body_text) > 500: break
        
        if len(body_text) < 500:
            paragraphs = soup.find_all("p")
            valid_paragraphs = [
                p.get_text(strip=True) for p in paragraphs
                if len(p.get_text(strip=True)) > 40
            ]
            body_text = "\n".join(valid_paragraphs)

        # â”€â”€ ì¡ìŒ í…ìŠ¤íŠ¸ í•„í„°ë§ (ê°•í™”ë¨) â”€â”€
        noise_patterns = [
            r'(?i)subscribe.*?newsletter',
            r'(?i)sign up for.*?email',
            r'(?i)get your.*?ticket',
            r'(?i)secure your spot.*?summit',
            r'(?i)digiday media buying summit.*?\.',
            r'(?i)subscribe to continue reading',
            r'(?i)subscription only',
            r'(?i)become a member',
            r'(?i)already a subscriber',
            r'(?i)read more about.*?membership',
            r'(?i)all rights reserved',
            r'(?i)copyright \d{4}',
            r'(?i)advertisement',
            r'(?i)follow us on',
            # AdAge/Digiday íŠ¹ì • ë¬¸êµ¬
            r'(?i)answers to common questions brands might have about the nation.*?s semiquincentennial',
            r'(?i)play them all',
            r'(?i)future of marketing briefing',
            r'(?i)latest marketing briefing',
        ]
        for pattern in noise_patterns:
            body_text = re.sub(pattern, '', body_text)

        lines = body_text.split('\n')
        clean_lines = []
        for line in lines:
            line_strip = line.strip()
            if len(line_strip) < 30: continue
            if line_strip.lower().startswith(('author:', 'by:', 'written by:', 'published:', 'source:')): continue
            # íŠ¹ì • ë°˜ë³µ ë¬¸êµ¬ ì œê±°
            if "digiday media buying summit" in line_strip.lower(): continue
            if "answers to common questions brands might have" in line_strip.lower(): continue
            
            clean_lines.append(line_strip)
        
        body_text = '\n\n'.join(clean_lines)
        return body_text[:4000]

    except Exception as e:
        logger.debug(f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒì„¸ ìš”ì•½ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_improved_summary(item, fulltext):
    title = item.get("title", "")
    rss_summary = item.get("summary", "")
    
    if fulltext and len(fulltext) > 300:
        source_text = fulltext[:1500]
    elif rss_summary and len(rss_summary) > 100:
        source_text = rss_summary
    else:
        source_text = f"{title}. {fulltext[:500]}" if fulltext else title

    kr_text = translate(source_text)

    # í•œêµ­ì–´ ì¡ìŒ ì œê±°
    noise_phrases = [
        "êµ¬ë… ì „ìš©", "ë‰´ìŠ¤ë ˆí„° ì‹ ì²­", "ìë¦¬ë¥¼ í™•ë³´í•˜ì„¸ìš”", 
        "êµ­ê°€ì˜ 500ì£¼ë…„ì— ëŒ€í•´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤", 
        "ëª¨ë‘ ì¬ìƒí•´ ë³´ì„¸ìš”", "Digiday Media Buying Summit"
    ]
    for np in noise_phrases:
        kr_text = kr_text.replace(np, "")

    kr_text = kr_text.replace("&quot;", "'").replace("&#39;", "'").strip()
    
    sentences = re.split(r'(?<=[.?!])\s+', kr_text)
    valid_sentences = []
    seen = set()
    for s in sentences:
        s = s.strip()
        if len(s) < 10: continue
        if s in seen: continue
        if s.startswith(("202", "ì‘ì„±ì:", "ì‚¬ì§„:", "ì´ë¯¸ì§€:")): continue
        seen.add(s)
        valid_sentences.append(s)
    
    summary_sentences = valid_sentences[:6]
    
    if summary_sentences:
        last_s = summary_sentences[-1]
        if not last_s.endswith(('.', '?', '!')):
            summary_sentences.pop()
            
    final_summary = ' '.join(summary_sentences)
    
    if len(final_summary) < 50:
        final_summary = translate(title) + ". (ìƒì„¸ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ì›ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”.)"
        
    return final_summary


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¶„ë¥˜ ë° ì„ ë³„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOPIC_KOREAN = {
    "AI/ìë™í™”": {"title": "AI ë° ìë™í™” ê¸°ìˆ ", "emoji": "ğŸ¤–"},
    "í”„ë¡œê·¸ë˜ë§¤í‹±/ì• ë“œí…Œí¬": {"title": "í”„ë¡œê·¸ë˜ë§¤í‹± & ì• ë“œí…Œí¬", "emoji": "âš™ï¸"},
    "ì†Œì…œë¯¸ë””ì–´": {"title": "ì†Œì…œë¯¸ë””ì–´ & í”Œë«í¼", "emoji": "ğŸ“±"},
    "ë™ì˜ìƒ/CTV": {"title": "ë™ì˜ìƒ/CTV/OTT", "emoji": "ğŸ¬"},
    "ëª¨ë°”ì¼": {"title": "ëª¨ë°”ì¼ & ì•± ë§ˆì¼€íŒ…", "emoji": "ğŸ“²"},
    "ë°ì´í„°/í”„ë¼ì´ë²„ì‹œ": {"title": "ë°ì´í„° & í”„ë¼ì´ë²„ì‹œ", "emoji": "ğŸ”’"},
    "ë¸Œëœë“œ/í¬ë¦¬ì—ì´í‹°ë¸Œ": {"title": "ë¸Œëœë“œ & í¬ë¦¬ì—ì´í‹°ë¸Œ", "emoji": "ğŸ¨"},
    "ì¸¡ì •/íš¨ê³¼": {"title": "íš¨ê³¼ ì¸¡ì • & ë¶„ì„", "emoji": "ğŸ“Š"},
    "ë¦¬í…Œì¼ë¯¸ë””ì–´": {"title": "ë¦¬í…Œì¼ ë¯¸ë””ì–´", "emoji": "ğŸ›’"},
}

def categorize_item_best(item):
    text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
    best_cat, best_score = "ê¸°íƒ€", 0
    for category, keywords in TREND_CATEGORIES.items():
        if category == "ê¸°íƒ€": continue
        score = sum(1 for kw in keywords if kw in text)
        if category == "AI/ìë™í™”": score *= 1.5
        if score > best_score:
            best_score = score
            best_cat = category
    return best_cat


def relevance_score(item):
    text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
    core_kw = [
        "advertis", "marketing", "media", "campaign", "brand", "consumer",
        "digital", "tech", "data", "platform", "content", "strategy", "research", "study", "analysis"
    ]
    score = sum(1 for kw in core_kw if kw in text)
    if item.get("type") == "academic": score += 3
    if "ai" in text or "gpt" in text or "generative" in text: score += 2
    return score


def select_top_items(articles, papers):
    articles_sorted = sorted(articles, key=lambda x: -relevance_score(x))
    papers_sorted = sorted(papers, key=lambda x: -relevance_score(x))
    
    final_articles = []
    source_counts = {}
    for a in articles_sorted:
        s = a.get("source", "unknown")
        if source_counts.get(s, 0) >= 3: continue
        final_articles.append(a)
        source_counts[s] = source_counts.get(s, 0) + 1
        if len(final_articles) >= 12: break # ê¸°ì‚¬ ìˆ˜ ì¡°ê¸ˆ ëŠ˜ë¦¼
        
    final_papers = papers_sorted[:8]
    return final_articles, final_papers


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³´ê³ ì„œ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_report(articles, papers):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    top_articles, top_papers = select_top_items(articles, papers)
    
    detail_map = {}
    all_items = top_articles + top_papers
    logger.info(f"ë¶„ì„ ëŒ€ìƒ: ê¸°ì‚¬ {len(top_articles)}ê±´, ë…¼ë¬¸ {len(top_papers)}ê±´")
    
    for idx, item in enumerate(all_items, 1):
        title = item.get("title", "")
        url = item.get("url", "")
        is_academic = (item.get("type") == "academic")
        logger.info(f"[{idx}/{len(all_items)}] ë¶„ì„ ì¤‘: {title[:30]}...")
        
        fulltext = ""
        if not is_academic:
            fulltext = fetch_article_fulltext(url)
        else:
            fulltext = item.get("summary", "") 
            
        summary = create_improved_summary(item, fulltext)
        detail_map[title] = summary

    categorized = {}
    for item in all_items:
        cat = categorize_item_best(item)
        categorized.setdefault(cat, []).append(item)
        
    report = f"""# ğŸ“¢ ì£¼ê°„ ê´‘ê³ /ë¯¸ë””ì–´ ì‹¬ì¸µ ë¦¬í¬íŠ¸

> **ë°œí–‰ì¼**: {now}
> **ì£¼ìš” ë‚´ìš©**: ì—…ê³„ ìµœì‹  ë‰´ìŠ¤ ë° **í•µì‹¬ í•™ìˆ  ì—°êµ¬** ë¶„ì„

---

## ğŸ’¡ ì´ë²ˆ ì£¼ í•µì‹¬ ìš”ì•½ (Executive Summary)

"""
    ai_items = categorized.get("AI/ìë™í™”", [])
    if ai_items:
        top_ai = ai_items[0]
        report += f"### ğŸ¤– AI íŠ¸ë Œë“œ: {translate(top_ai.get('title'))}\n"
        report += f"{detail_map.get(top_ai.get('title'))}\n\n"
        
    papers_only = [i for i in all_items if i.get("type") == "academic"]
    if papers_only:
        top_paper = papers_only[0]
        report += f"### ğŸ“š ì£¼ëª©í•  ì—°êµ¬: {translate(top_paper.get('title'))}\n"
        report += f"{detail_map.get(top_paper.get('title'))}\n\n"

    report += "---\n\n"



    report += "## ğŸ“° ì—…ê³„ ì£¼ìš” íŠ¸ë Œë“œ (Industry News)\n\n"
    sorted_cats = sorted(categorized.keys(), key=lambda c: (c=="AI/ìë™í™”", len(categorized[c])), reverse=True)
    
    for cat in sorted_cats:
        items = [i for i in categorized[cat] if i.get("type") == "industry"]
        if not items: continue
        
        info = TOPIC_KOREAN.get(cat, {"title": cat, "emoji": "ğŸ”¹"})
        report += f"### {info['emoji']} {info['title']}\n\n"
        
        for item in items:
            title_kr = translate(item.get("title"))
            summary_kr = detail_map.get(item.get("title"), "")
            source = item.get("source", "")
            url = item.get("url", "")
            
            report += f"**{title_kr}**\n"
            report += f"*({source})*\n\n"
            report += f"{summary_kr}\n\n"
            if url:
                report += f"[ğŸ”— ê¸°ì‚¬ ì›ë¬¸]({url})\n\n"
        
        report += "\n"

    report += "---\n\n"

    report += "## ğŸ“š ìµœì‹  í•™ìˆ  ì—°êµ¬ ë° ì´ë¡  (Research & Theory)\n\n"
    if papers_only:
        for p in papers_only:
            title_kr = translate(p.get("title", ""))
            summary_kr = detail_map.get(p.get("title", ""), "")
            source = p.get("source", "Academic Source")
            url = p.get("url", "")
            
            report += f"### {title_kr}\n"
            report += f"*ì¶œì²˜: {source}*\n\n"
            if summary_kr:
                report += f"{summary_kr}\n\n"
            if url:
                report += f"[ğŸ‘‰ ë…¼ë¬¸/URL í™•ì¸]({url})\n\n"
            report += "---\n\n"
    else:
        report += "> *ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ì£¼ìš” í•™ìˆ  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.*\n\n"

    report += f"""---
> **[ì•ˆë‚´]** ë³¸ ë³´ê³ ì„œëŠ” ìë™í™”ëœ ì‹œìŠ¤í…œì— ì˜í•´ ìˆ˜ì§‘ ë° ìš”ì•½ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
> ìƒì„± ì‹œê°„: {now}
"""
    return report


def create_report():
    data_dir = get_today_data_dir()
    articles, papers = [], []
    try:
        with open(data_dir / "raw_articles.json", "r", encoding="utf-8") as f:
            articles = json.load(f)
        with open(data_dir / "raw_papers.json", "r", encoding="utf-8") as f:
            papers = json.load(f)
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
        
    if not articles and not papers:
        logger.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    logger.info("ë³´ê³ ì„œ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    report_content = generate_report(articles, papers)
    
    report_path = get_today_report_path()
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)
        
    return str(report_path)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S")
    path = create_report()
    if path: print(f"Report Created: {path}")
